{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Woof_128_twist.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "LP3vVSWMBz1L",
        "6C-1pE45Bz1l",
        "pLmt0ISPBz2w"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liuyao12/imagenette_experiments/blob/master/ResNet_twist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM0cNcW2Bzz4",
        "colab_type": "text"
      },
      "source": [
        "# ResNet with a Twist\n",
        "\n",
        "> with depthwise (x4) + Ranger + Mish + SA + MaxBlurPool + ResTrick\n",
        "\n",
        "See blog https://liuyao12.github.io/blog/research/2020/03/07/Conv-Twist.html\n",
        "\n",
        "See summary at https://forums.fast.ai/t/imagenette-imagewoof-leaderboards/45822/47?u=liuyao "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXBTriT1fmXE",
        "colab_type": "text"
      },
      "source": [
        "## Imagewoof Leaderboard\n",
        "\n",
        "Imagewoof2, with a 70/30 train/test ratio.\n",
        "\n",
        "| Size (px) | Epochs | SoTA| x2 | x4 | x4 twist | x6 | x4 double | runs |\n",
        "|--|--|--| --|--| --|--|--|--|\n",
        "|128|5|73.37|75.19|76.27||76.61| **82.12**|5, mean\n",
        "|128|20|85.52|85.18|86.22||86.27| **88.93**|5, mean\n",
        "|128|80|87.20|87.70|87.83||87.65| **90.15**|1\n",
        "|128|200|87.20|\n",
        "|192|5|77.87|79.86|81.15|80.73|| **82.69**|5, mean\n",
        "|192|20|87.85|88.12|88.37|88.28|\n",
        "|192|80|89.21|90.30|89.89|89.38|| **92.08** |\n",
        "|192|200|89.54\n",
        "|256|5|\n",
        "|256|20|\n",
        "|256|80|\n",
        "|256|200|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrD1-te_Bzz7",
        "colab_type": "text"
      },
      "source": [
        "# setup and imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtfoQmHrJonm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pip install kornia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UteAROmqBzz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install git+https://github.com/ayasyrev/model_constructor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XnsPVNrkTfV3",
        "colab": {}
      },
      "source": [
        "pip install git+https://github.com/ayasyrev/imagenette_experiments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-OCjxGCBz0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai.basic_train import *\n",
        "from fastai.vision import *\n",
        "# from fastai.script import *"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPNF3XKWBz0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from kornia.contrib import MaxBlurPool2d"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPGwrk6TBz0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from imagenette_experiments.train_utils import *\n",
        "from model_constructor.net import Net, act_fn\n",
        "from model_constructor.layers import SimpleSelfAttention, ConvLayer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U63Mcb1lBz0p",
        "colab_type": "text"
      },
      "source": [
        "# ResBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbKrwk-649_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MnM(nn.Module): # Mix and Multiply\n",
        "    def __init__(self, channels, group_size):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.gs = group_size\n",
        "        n = channels//group_size*2\n",
        "        self.conv = nn.Conv2d(n, n*2, 1, groups=n, bias=True)\n",
        "        self.XY = None\n",
        "\n",
        "    def forward(self, x): \n",
        "        N,C,H,W = x.size()\n",
        "        # x1 = x.view(N,-1,self.gs,H,W)[:,:,:-2].reshape(N,-1,H,W)\n",
        "        x2 = x.view(N,-1,self.gs,H,W)[:,:,-2:].reshape(N,-1,H,W)\n",
        "        if self.XY is None:\n",
        "            XX = torch.from_numpy(np.indices((1,H,W))[2]*2/W-1)\n",
        "            YY = torch.from_numpy(np.indices((1,H,W))[1]*2/H-1)\n",
        "            g = self.channels//self.gs*2\n",
        "            self.XY = torch.cat([XX,YY]*g, dim=0).to(x.device).type(x.dtype)\n",
        "        twist = self.conv(x2)*self.XY\n",
        "        twist = torch.sum(twist.view(N,-1,2,2,H,W), dim=2).reshape(N,-1,H,W)\n",
        "        return torch.cat([x, twist], dim=1)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kY7y9c99Bz0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NewLayer(nn.Sequential):\n",
        "    \"\"\"Basic conv layers block\"\"\"\n",
        "    def __init__(self, ni, nf, ks=3, stride=1,\n",
        "            act=True,  act_fn=nn.ReLU(inplace=True),\n",
        "            bn_layer=True, bn_1st=True, zero_bn=False,\n",
        "            padding=None, bias=False, groups=1, **kwargs):\n",
        "\n",
        "        if padding==None: padding = ks//2\n",
        "        if ks==3 and groups==1:  # to be used for the \"stem\" of ResNet\n",
        "          # if ni==3: stride = 2\n",
        "          layers = [('Conv3x3', nn.Conv2d(ni, ni*dm, 3, stride=stride, padding=1, bias=bias, groups=ni)),\n",
        "                    ('Conv1x1', nn.Conv2d(ni*dm, nf, 1, bias=bias, groups=1))]\n",
        "        else:\n",
        "          layers = [('Conv{}x{}'.format(ks,ks), \n",
        "                      nn.Conv2d(ni, nf, ks, stride=stride, padding=padding, bias=bias, groups=groups))]\n",
        "\n",
        "        act_bn = [('act_fn', act_fn)] if act else []\n",
        "        if bn_layer:\n",
        "            bn = nn.BatchNorm2d(nf)\n",
        "            nn.init.constant_(bn.weight, 0. if zero_bn else 1.)\n",
        "            act_bn += [('bn', bn)]\n",
        "        if bn_1st: act_bn.reverse()\n",
        "        layers += act_bn\n",
        "        super().__init__(OrderedDict(layers))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVLFz9nhBz0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NewResBlock(Module):\n",
        "    def __init__(self, expansion, ni, nh, stride=1,\n",
        "                 conv_layer=ConvLayer, act_fn=act_fn, zero_bn=True, bn_1st=True,\n",
        "                 pool=nn.AvgPool2d(2, ceil_mode=True), sa=False, sym=False, groups=1):\n",
        "        nf,ni = nh*expansion,ni*expansion\n",
        "        conv_layer = NewLayer\n",
        "        self.reduce = noop if stride==1 else pool\n",
        "        layers  = [(f\"conv_0\", conv_layer(ni, nh, 3, act_fn=act_fn, bn_1st=bn_1st)),\n",
        "                   (f\"conv_1\", conv_layer(ni, nf, 3, zero_bn=zero_bn, act=False, bn_layer=True))\n",
        "        ] if expansion == 1 else [\n",
        "                   (f\"conv_0\", conv_layer(ni, nh, 1, act_fn=act_fn, bn_1st=bn_1st)),\n",
        "                   # (f\"conv_1\", conv_layer(nh, nh, 3, act_fn=act_fn, bn_1st=bn_1st)),\n",
        "                   # (f\"conv_1\", conv_layer(nh, nh*dm, 3, groups=nh, act_fn=act_fn, bn_1st=bn_1st)),\n",
        "                   (f\"conv_1\", conv_layer(nh, nh*dm, 3, groups=nh, act=False, bn_layer=False)),\n",
        "                   (f\"MnM\", MnM(nf, dm)),\n",
        "                   (f\"conv_2\", conv_layer(nh*(dm+2), nf, 1, zero_bn=zero_bn, act=False, bn_1st=bn_1st))\n",
        "        ]\n",
        "        if sa: layers.append(('sa', SimpleSelfAttention(nf,ks=1,sym=sym)))\n",
        "        self.convs = nn.Sequential(OrderedDict(layers))\n",
        "        self.idconv = noop if ni==nf else conv_layer(ni, nf, 1, act=False, bn_1st=bn_1st)\n",
        "        self.merge = act_fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        o = self.reduce(x)\n",
        "        return self.merge(self.convs(o) + self.idconv(o))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XroIp4GcBz07",
        "colab_type": "text"
      },
      "source": [
        "# Model Constructor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTrZVV81Bz1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Net(c_out=10, layers=[3,6,8,3], expansion=4)\n",
        "model.block = NewResBlock\n",
        "model.conv_layer = NewLayer # for the stem\n",
        "pool = MaxBlurPool2d(3, True)\n",
        "model.pool = pool\n",
        "model.stem_sizes = [3,32,64,64]\n",
        "model.act_fn = Mish()\n",
        "model.sa = True"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCDxE73yuq_5",
        "colab_type": "text"
      },
      "source": [
        "## Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JfDW5iqPVE4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4c5f02dc-d651-4206-cdcf-8c8a07894e7b"
      },
      "source": [
        "dm = 4\n",
        "res = dict()\n",
        "for ep in [5]*5 + [20] + [80]:\n",
        "    mixup=0 if ep<=20 else 0.2\n",
        "    learn = get_learn(model=model, size=192, bs=16, mixup=mixup)\n",
        "    learn.fit_fc(ep, lr=4e-3, moms=(0.95,0.95), start_pct=0.72)\n",
        "    acc = learn.recorder.metrics[-1][0].item()\n",
        "    res[ep] = [acc] if ep not in res else res[ep] + [acc]\n",
        "    print('{} epochs: {} ({} runs)'.format(ep, sum(res[ep])/len(res[ep]), len(res[ep])))\n",
        "print('depth multiplier={}'.format(dm), {ep: sum(res[ep])/len(res[ep]) for ep in res})"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data path   /root/.fastai/data/imagewoof2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Learn path /root/.fastai/data/imagewoof2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>top_k_accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.905540</td>\n",
              "      <td>1.814509</td>\n",
              "      <td>0.409519</td>\n",
              "      <td>0.880377</td>\n",
              "      <td>02:27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.671987</td>\n",
              "      <td>1.491707</td>\n",
              "      <td>0.561721</td>\n",
              "      <td>0.935607</td>\n",
              "      <td>02:27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.451532</td>\n",
              "      <td>1.294970</td>\n",
              "      <td>0.665309</td>\n",
              "      <td>0.954950</td>\n",
              "      <td>02:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.311999</td>\n",
              "      <td>1.187245</td>\n",
              "      <td>0.720031</td>\n",
              "      <td>0.964113</td>\n",
              "      <td>02:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.119960</td>\n",
              "      <td>1.026220</td>\n",
              "      <td>0.810639</td>\n",
              "      <td>0.978875</td>\n",
              "      <td>02:28</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "5 epochs: 0.8106388449668884 (1 runs)\n",
            "data path   /root/.fastai/data/imagewoof2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Learn path /root/.fastai/data/imagewoof2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>top_k_accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.882510</td>\n",
              "      <td>1.744457</td>\n",
              "      <td>0.441079</td>\n",
              "      <td>0.902265</td>\n",
              "      <td>02:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.633589</td>\n",
              "      <td>1.500231</td>\n",
              "      <td>0.574701</td>\n",
              "      <td>0.931026</td>\n",
              "      <td>02:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.409354</td>\n",
              "      <td>1.397438</td>\n",
              "      <td>0.613642</td>\n",
              "      <td>0.951896</td>\n",
              "      <td>02:27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.297217</td>\n",
              "      <td>1.203432</td>\n",
              "      <td>0.718758</td>\n",
              "      <td>0.963095</td>\n",
              "      <td>02:27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.063921</td>\n",
              "      <td>1.034552</td>\n",
              "      <td>0.797149</td>\n",
              "      <td>0.976075</td>\n",
              "      <td>02:27</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "5 epochs: 0.8038941323757172 (2 runs)\n",
            "data path   /root/.fastai/data/imagewoof2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Learn path /root/.fastai/data/imagewoof2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>top_k_accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.846126</td>\n",
              "      <td>1.748649</td>\n",
              "      <td>0.444897</td>\n",
              "      <td>0.896666</td>\n",
              "      <td>02:27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.614222</td>\n",
              "      <td>1.568512</td>\n",
              "      <td>0.548231</td>\n",
              "      <td>0.930517</td>\n",
              "      <td>02:27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.448857</td>\n",
              "      <td>1.347386</td>\n",
              "      <td>0.643166</td>\n",
              "      <td>0.953169</td>\n",
              "      <td>02:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.310275</td>\n",
              "      <td>1.211324</td>\n",
              "      <td>0.711886</td>\n",
              "      <td>0.965640</td>\n",
              "      <td>02:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.120510</td>\n",
              "      <td>1.031483</td>\n",
              "      <td>0.798931</td>\n",
              "      <td>0.981675</td>\n",
              "      <td>02:25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5 epochs: 0.8022397557894388 (3 runs)\n",
            "data path   /root/.fastai/data/imagewoof2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Learn path /root/.fastai/data/imagewoof2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>top_k_accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.863362</td>\n",
              "      <td>1.717335</td>\n",
              "      <td>0.453041</td>\n",
              "      <td>0.895139</td>\n",
              "      <td>02:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.604840</td>\n",
              "      <td>1.443263</td>\n",
              "      <td>0.587936</td>\n",
              "      <td>0.941715</td>\n",
              "      <td>02:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.435408</td>\n",
              "      <td>1.284409</td>\n",
              "      <td>0.670400</td>\n",
              "      <td>0.959532</td>\n",
              "      <td>02:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.276258</td>\n",
              "      <td>1.154402</td>\n",
              "      <td>0.737847</td>\n",
              "      <td>0.968949</td>\n",
              "      <td>02:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.065301</td>\n",
              "      <td>1.009147</td>\n",
              "      <td>0.809875</td>\n",
              "      <td>0.977857</td>\n",
              "      <td>02:26</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5 epochs: 0.8041486442089081 (4 runs)\n",
            "data path   /root/.fastai/data/imagewoof2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Learn path /root/.fastai/data/imagewoof2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='2' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      40.00% [2/5 04:52<07:18]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>top_k_accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.905647</td>\n",
              "      <td>1.734021</td>\n",
              "      <td>0.446933</td>\n",
              "      <td>0.899466</td>\n",
              "      <td>02:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.592445</td>\n",
              "      <td>1.469542</td>\n",
              "      <td>0.567574</td>\n",
              "      <td>0.941715</td>\n",
              "      <td>02:26</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='444' class='' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      78.72% [444/564 01:31<00:24 1.4434]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj0OrEg8rEuk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "1ed9fd65-7640-484b-8bc4-8d3bbf0c2202"
      },
      "source": [
        "print(res)\n",
        "print('depth multiplier={}'.format(dm), {e: sum(res[e])/len(res[e]) for e in res})"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{5: [0.8106388449668884, 0.7971494197845459, 0.7989310026168823, 0.8098753094673157, 0.8088572025299072], 20: [0.8875032067298889], 80: [0.8946296572685242]}\n",
            "depth multiplier=4 {5: 0.805090355873108, 20: 0.8875032067298889, 80: 0.8946296572685242}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOZwNYzv-hcp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "86cb9cc5-25ae-43f5-8763-c13b84a9ef74"
      },
      "source": [
        "for e in [200]: #*5 + [20] + [80]:\n",
        "  mixup=0 if e<=20 else 0.2\n",
        "  learn = get_learn(model=model, size=192, bs=16, mixup=mixup)\n",
        "  learn.fit_fc(e, lr=4e-3, moms=(0.95,0.95), start_pct=0.72)\n",
        "  acc = learn.recorder.metrics[-1][0].item()\n",
        "  if e in res:\n",
        "    res[e] += [acc]\n",
        "  else:\n",
        "    res[e] = [acc]\n",
        "  print('{} epochs: {} ({} runs)'.format(e, sum(res[e])/len(res[e]), len(res[e])))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data path   /root/.fastai/data/imagewoof2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Learn path /root/.fastai/data/imagewoof2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='82' class='' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      41.00% [82/200 3:50:07<5:31:09]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>top_k_accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.028049</td>\n",
              "      <td>1.774158</td>\n",
              "      <td>0.426063</td>\n",
              "      <td>0.880886</td>\n",
              "      <td>02:50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.813608</td>\n",
              "      <td>1.551716</td>\n",
              "      <td>0.544922</td>\n",
              "      <td>0.925426</td>\n",
              "      <td>02:50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.717604</td>\n",
              "      <td>1.439224</td>\n",
              "      <td>0.607279</td>\n",
              "      <td>0.939679</td>\n",
              "      <td>02:50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.615500</td>\n",
              "      <td>1.326114</td>\n",
              "      <td>0.659455</td>\n",
              "      <td>0.949096</td>\n",
              "      <td>02:50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.551526</td>\n",
              "      <td>1.227806</td>\n",
              "      <td>0.700687</td>\n",
              "      <td>0.959277</td>\n",
              "      <td>02:50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.520333</td>\n",
              "      <td>1.218766</td>\n",
              "      <td>0.706032</td>\n",
              "      <td>0.963349</td>\n",
              "      <td>02:50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.489147</td>\n",
              "      <td>1.153190</td>\n",
              "      <td>0.742683</td>\n",
              "      <td>0.966913</td>\n",
              "      <td>02:49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.444332</td>\n",
              "      <td>1.141110</td>\n",
              "      <td>0.743446</td>\n",
              "      <td>0.966149</td>\n",
              "      <td>02:49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.384485</td>\n",
              "      <td>1.117875</td>\n",
              "      <td>0.753118</td>\n",
              "      <td>0.968949</td>\n",
              "      <td>02:49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.392756</td>\n",
              "      <td>1.119168</td>\n",
              "      <td>0.750827</td>\n",
              "      <td>0.967931</td>\n",
              "      <td>02:49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.350738</td>\n",
              "      <td>1.081199</td>\n",
              "      <td>0.769407</td>\n",
              "      <td>0.974548</td>\n",
              "      <td>02:49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.329751</td>\n",
              "      <td>1.087370</td>\n",
              "      <td>0.766098</td>\n",
              "      <td>0.969967</td>\n",
              "      <td>02:49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.289628</td>\n",
              "      <td>1.019599</td>\n",
              "      <td>0.797149</td>\n",
              "      <td>0.974039</td>\n",
              "      <td>02:49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.280720</td>\n",
              "      <td>1.033650</td>\n",
              "      <td>0.793332</td>\n",
              "      <td>0.971494</td>\n",
              "      <td>02:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.262636</td>\n",
              "      <td>0.992098</td>\n",
              "      <td>0.807330</td>\n",
              "      <td>0.975312</td>\n",
              "      <td>02:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.248712</td>\n",
              "      <td>1.038021</td>\n",
              "      <td>0.787223</td>\n",
              "      <td>0.976075</td>\n",
              "      <td>02:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.219651</td>\n",
              "      <td>1.002507</td>\n",
              "      <td>0.801985</td>\n",
              "      <td>0.978366</td>\n",
              "      <td>02:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.220358</td>\n",
              "      <td>0.993956</td>\n",
              "      <td>0.803258</td>\n",
              "      <td>0.978366</td>\n",
              "      <td>02:49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.239965</td>\n",
              "      <td>0.999850</td>\n",
              "      <td>0.806058</td>\n",
              "      <td>0.974039</td>\n",
              "      <td>02:49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.174423</td>\n",
              "      <td>1.008343</td>\n",
              "      <td>0.799949</td>\n",
              "      <td>0.975312</td>\n",
              "      <td>02:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.178095</td>\n",
              "      <td>0.968589</td>\n",
              "      <td>0.819038</td>\n",
              "      <td>0.979130</td>\n",
              "      <td>02:49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.173939</td>\n",
              "      <td>0.957028</td>\n",
              "      <td>0.820056</td>\n",
              "      <td>0.977857</td>\n",
              "      <td>02:49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.139889</td>\n",
              "      <td>0.994223</td>\n",
              "      <td>0.801222</td>\n",
              "      <td>0.974548</td>\n",
              "      <td>02:49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.155432</td>\n",
              "      <td>0.947193</td>\n",
              "      <td>0.825655</td>\n",
              "      <td>0.978366</td>\n",
              "      <td>02:49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.130020</td>\n",
              "      <td>0.954224</td>\n",
              "      <td>0.822601</td>\n",
              "      <td>0.974294</td>\n",
              "      <td>02:49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.109196</td>\n",
              "      <td>0.943498</td>\n",
              "      <td>0.820565</td>\n",
              "      <td>0.980148</td>\n",
              "      <td>02:49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.099459</td>\n",
              "      <td>0.963224</td>\n",
              "      <td>0.816747</td>\n",
              "      <td>0.974548</td>\n",
              "      <td>02:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.101055</td>\n",
              "      <td>0.914036</td>\n",
              "      <td>0.836600</td>\n",
              "      <td>0.983202</td>\n",
              "      <td>02:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.100304</td>\n",
              "      <td>0.941273</td>\n",
              "      <td>0.832527</td>\n",
              "      <td>0.982438</td>\n",
              "      <td>02:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.049737</td>\n",
              "      <td>0.935719</td>\n",
              "      <td>0.831000</td>\n",
              "      <td>0.978366</td>\n",
              "      <td>02:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.104826</td>\n",
              "      <td>0.936456</td>\n",
              "      <td>0.828455</td>\n",
              "      <td>0.982438</td>\n",
              "      <td>02:49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.081744</td>\n",
              "      <td>0.904647</td>\n",
              "      <td>0.843472</td>\n",
              "      <td>0.979130</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.061258</td>\n",
              "      <td>0.913190</td>\n",
              "      <td>0.841435</td>\n",
              "      <td>0.980402</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.032333</td>\n",
              "      <td>0.910069</td>\n",
              "      <td>0.848053</td>\n",
              "      <td>0.975821</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.025491</td>\n",
              "      <td>0.914826</td>\n",
              "      <td>0.837618</td>\n",
              "      <td>0.979130</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.018122</td>\n",
              "      <td>0.913928</td>\n",
              "      <td>0.840926</td>\n",
              "      <td>0.982184</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.033709</td>\n",
              "      <td>0.917805</td>\n",
              "      <td>0.834818</td>\n",
              "      <td>0.977348</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.042897</td>\n",
              "      <td>0.916017</td>\n",
              "      <td>0.837109</td>\n",
              "      <td>0.977348</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.033942</td>\n",
              "      <td>0.912502</td>\n",
              "      <td>0.838636</td>\n",
              "      <td>0.979130</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.042509</td>\n",
              "      <td>0.920061</td>\n",
              "      <td>0.836345</td>\n",
              "      <td>0.979639</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.023923</td>\n",
              "      <td>0.910699</td>\n",
              "      <td>0.842454</td>\n",
              "      <td>0.979639</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.004876</td>\n",
              "      <td>0.898885</td>\n",
              "      <td>0.847544</td>\n",
              "      <td>0.981166</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.002666</td>\n",
              "      <td>0.894847</td>\n",
              "      <td>0.843981</td>\n",
              "      <td>0.979893</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.996454</td>\n",
              "      <td>0.912025</td>\n",
              "      <td>0.842963</td>\n",
              "      <td>0.979893</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.011663</td>\n",
              "      <td>0.901190</td>\n",
              "      <td>0.843981</td>\n",
              "      <td>0.980402</td>\n",
              "      <td>02:46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.990995</td>\n",
              "      <td>0.896265</td>\n",
              "      <td>0.849326</td>\n",
              "      <td>0.980148</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.960949</td>\n",
              "      <td>0.929989</td>\n",
              "      <td>0.827182</td>\n",
              "      <td>0.978875</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.997194</td>\n",
              "      <td>0.881046</td>\n",
              "      <td>0.853652</td>\n",
              "      <td>0.981420</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.987275</td>\n",
              "      <td>0.935693</td>\n",
              "      <td>0.835836</td>\n",
              "      <td>0.978621</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.941286</td>\n",
              "      <td>0.896995</td>\n",
              "      <td>0.841181</td>\n",
              "      <td>0.980148</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.963668</td>\n",
              "      <td>0.890014</td>\n",
              "      <td>0.853907</td>\n",
              "      <td>0.981675</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.974081</td>\n",
              "      <td>0.915690</td>\n",
              "      <td>0.846271</td>\n",
              "      <td>0.980657</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.976538</td>\n",
              "      <td>0.900991</td>\n",
              "      <td>0.845762</td>\n",
              "      <td>0.981166</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.979728</td>\n",
              "      <td>0.904955</td>\n",
              "      <td>0.844744</td>\n",
              "      <td>0.975821</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.954007</td>\n",
              "      <td>0.906955</td>\n",
              "      <td>0.843726</td>\n",
              "      <td>0.976075</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.980041</td>\n",
              "      <td>0.899640</td>\n",
              "      <td>0.842963</td>\n",
              "      <td>0.979639</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.963368</td>\n",
              "      <td>0.920975</td>\n",
              "      <td>0.844744</td>\n",
              "      <td>0.974294</td>\n",
              "      <td>02:46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.964298</td>\n",
              "      <td>0.910518</td>\n",
              "      <td>0.844235</td>\n",
              "      <td>0.977093</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.952001</td>\n",
              "      <td>0.906925</td>\n",
              "      <td>0.839399</td>\n",
              "      <td>0.979130</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.927256</td>\n",
              "      <td>0.877739</td>\n",
              "      <td>0.852380</td>\n",
              "      <td>0.981929</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.934466</td>\n",
              "      <td>0.902497</td>\n",
              "      <td>0.845508</td>\n",
              "      <td>0.977348</td>\n",
              "      <td>02:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.955934</td>\n",
              "      <td>0.911812</td>\n",
              "      <td>0.840672</td>\n",
              "      <td>0.975821</td>\n",
              "      <td>02:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.930215</td>\n",
              "      <td>0.920043</td>\n",
              "      <td>0.832273</td>\n",
              "      <td>0.976330</td>\n",
              "      <td>02:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.937125</td>\n",
              "      <td>0.888367</td>\n",
              "      <td>0.849326</td>\n",
              "      <td>0.982947</td>\n",
              "      <td>02:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.917101</td>\n",
              "      <td>0.888784</td>\n",
              "      <td>0.854416</td>\n",
              "      <td>0.982438</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.926207</td>\n",
              "      <td>0.875418</td>\n",
              "      <td>0.853652</td>\n",
              "      <td>0.980657</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.952710</td>\n",
              "      <td>0.905040</td>\n",
              "      <td>0.843217</td>\n",
              "      <td>0.972767</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.953184</td>\n",
              "      <td>0.900884</td>\n",
              "      <td>0.841690</td>\n",
              "      <td>0.977348</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.945614</td>\n",
              "      <td>0.892319</td>\n",
              "      <td>0.847289</td>\n",
              "      <td>0.979639</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.931085</td>\n",
              "      <td>0.891037</td>\n",
              "      <td>0.855688</td>\n",
              "      <td>0.979893</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.914815</td>\n",
              "      <td>0.871394</td>\n",
              "      <td>0.853907</td>\n",
              "      <td>0.980402</td>\n",
              "      <td>02:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.943974</td>\n",
              "      <td>0.908905</td>\n",
              "      <td>0.836091</td>\n",
              "      <td>0.980402</td>\n",
              "      <td>02:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.926494</td>\n",
              "      <td>0.897443</td>\n",
              "      <td>0.847798</td>\n",
              "      <td>0.975566</td>\n",
              "      <td>02:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.915003</td>\n",
              "      <td>0.908796</td>\n",
              "      <td>0.843726</td>\n",
              "      <td>0.974548</td>\n",
              "      <td>02:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.907841</td>\n",
              "      <td>0.886548</td>\n",
              "      <td>0.847544</td>\n",
              "      <td>0.979130</td>\n",
              "      <td>02:49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.928801</td>\n",
              "      <td>0.900546</td>\n",
              "      <td>0.842199</td>\n",
              "      <td>0.976584</td>\n",
              "      <td>02:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.929743</td>\n",
              "      <td>0.903639</td>\n",
              "      <td>0.846526</td>\n",
              "      <td>0.978621</td>\n",
              "      <td>02:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.935913</td>\n",
              "      <td>0.889361</td>\n",
              "      <td>0.848562</td>\n",
              "      <td>0.979893</td>\n",
              "      <td>02:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.920459</td>\n",
              "      <td>0.929610</td>\n",
              "      <td>0.831000</td>\n",
              "      <td>0.977857</td>\n",
              "      <td>02:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.931280</td>\n",
              "      <td>0.887868</td>\n",
              "      <td>0.847798</td>\n",
              "      <td>0.980148</td>\n",
              "      <td>02:49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.921763</td>\n",
              "      <td>0.893713</td>\n",
              "      <td>0.848562</td>\n",
              "      <td>0.976584</td>\n",
              "      <td>02:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.919791</td>\n",
              "      <td>0.902908</td>\n",
              "      <td>0.845253</td>\n",
              "      <td>0.979639</td>\n",
              "      <td>02:48</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='16' class='' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      2.84% [16/564 00:04<02:21 0.9225]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-ae89c47725a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mmixup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m20\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_learn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m192\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmixup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmixup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_pct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.72\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/train.py\u001b[0m in \u001b[0;36mfit_fc\u001b[0;34m(learn, tot_epochs, lr, moms, start_pct, wd, callbacks)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatCosAnnealScheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmoms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_pct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_pct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtot_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtot_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mLearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_div\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_bwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip_bwd\u001b[0m\u001b[0;34m:\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/callback.py\u001b[0m in \u001b[0;36mon_backward_begin\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_backward_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;34m\"Handle gradient calculation on `loss`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmoothener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'smooth_loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmoothener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'backward_begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall_mets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euDz8pVe_PDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}