{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Woof_128_twist.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "LP3vVSWMBz1L",
        "6C-1pE45Bz1l",
        "pLmt0ISPBz2w"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liuyao12/imagenette_experiments/blob/master/ResNet_twist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM0cNcW2Bzz4",
        "colab_type": "text"
      },
      "source": [
        "# ResNet with a Twist, with depthwise separable convolution\n",
        "\n",
        "> depthwise (x4) + Ranger + Mish + SA + MaxBlurPool + ResTrick\n",
        "\n",
        "See blog https://liuyao12.github.io/blog/research/2020/03/07/Conv-Twist.html\n",
        "\n",
        "See summary at https://forums.fast.ai/t/imagenette-imagewoof-leaderboards/45822/47?u=liuyao "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXBTriT1fmXE",
        "colab_type": "text"
      },
      "source": [
        "## Imagewoof Leaderboard\n",
        "\n",
        "Imagewoof2, with a 70/30 train/test ratio.\n",
        "\n",
        "| Size (px) | Epochs | SoTA| x2 | x4 | x6 | x4 double | runs |\n",
        "|--|--|--| --|--| --|--|--|\n",
        "|128|5|73.37|75.19|76.27|76.61| **82.12**|5, mean\n",
        "|128|20|85.52|85.18|86.22|86.27| **88.93**|5, mean\n",
        "|128|80|87.20|87.70|87.83|87.65| **90.15**|1\n",
        "|128|200|87.20|\n",
        "|192|5|77.87|79.86|81.15|| **82.69**|5, mean\n",
        "|192|20|87.85|88.12|88.37|\n",
        "|192|80|89.21|90.30|89.89|| **92.08** |\n",
        "|192|200|89.54\n",
        "|256|5|\n",
        "|256|20|\n",
        "|256|80|\n",
        "|256|200|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrD1-te_Bzz7",
        "colab_type": "text"
      },
      "source": [
        "# setup and imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtfoQmHrJonm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pip install kornia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UteAROmqBzz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install git+https://github.com/ayasyrev/model_constructor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XnsPVNrkTfV3",
        "colab": {}
      },
      "source": [
        "pip install git+https://github.com/ayasyrev/imagenette_experiments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-OCjxGCBz0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai.basic_train import *\n",
        "from fastai.vision import *\n",
        "# from fastai.script import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPNF3XKWBz0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from kornia.contrib import MaxBlurPool2d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPGwrk6TBz0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from imagenette_experiments.train_utils import *\n",
        "from model_constructor.net import Net, act_fn\n",
        "from model_constructor.layers import SimpleSelfAttention, ConvLayer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U63Mcb1lBz0p",
        "colab_type": "text"
      },
      "source": [
        "# ResBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbKrwk-649_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MnM(nn.Module): # Mix and Multiply\n",
        "    def __init__(self, channels, group_size):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.gs = group_size\n",
        "        self.conv = nn.Conv2d(channels, channels, 1, groups=1, bias=False)\n",
        "        self.mask = torch.zeros((channels,channels,1,1))\n",
        "        for i in range(channels//group_size):\n",
        "            self.mask[i*dm:(i+1)*dm,i*dm:(i+1)*dm] = 1\n",
        "        self.XY = None\n",
        "\n",
        "    def forward(self, x): \n",
        "        if self.XY is None:\n",
        "            N,C,H,W = x.size()\n",
        "            XX = torch.from_numpy(np.indices((1,H,W))[2]*2/W-1)\n",
        "            YY = torch.from_numpy(np.indices((1,H,W))[1]*2/H-1)\n",
        "            ones = torch.ones((self.gs-2,H,W))\n",
        "            XY = torch.cat([ones, XX, YY], dim=0).to(x.device).type(x.dtype)\n",
        "            self.XY = XY.unsqueeze(0).expand(self.channels//self.gs,self.gs,H,W).reshape(self.channels,H,W)\n",
        "            self.mask = self.mask.to(x.device).type(x.dtype)\n",
        "        self.conv.weight.data = self.mask * self.conv.weight\n",
        "        return self.conv(x) * self.XY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kY7y9c99Bz0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NewLayer(nn.Sequential):\n",
        "    \"\"\"Basic conv layers block\"\"\"\n",
        "    def __init__(self, ni, nf, ks=3, stride=1,\n",
        "            act=True,  act_fn=nn.ReLU(inplace=True),\n",
        "            bn_layer=True, bn_1st=True, zero_bn=False,\n",
        "            padding=None, bias=False, groups=1, **kwargs):\n",
        "\n",
        "        if padding==None: padding = ks//2\n",
        "        if ks==3 and groups==1:  # to be used for the \"stem\" of ResNet\n",
        "          # if ni==3: stride = 2\n",
        "          layers = [('Conv3x3', nn.Conv2d(ni, ni*dm, 3, stride=stride, padding=1, bias=bias, groups=ni)),\n",
        "                    ('Conv1x1', nn.Conv2d(ni*dm, nf, 1, bias=bias, groups=1))]\n",
        "        else:\n",
        "          layers = [('Conv{}x{}'.format(ks,ks), \n",
        "                      nn.Conv2d(ni, nf, ks, stride=stride, padding=padding, bias=bias, groups=groups))]\n",
        "          if ks==3:\n",
        "            layers += [('M_op', MnM(nf, dm))]\n",
        "\n",
        "        act_bn = [('act_fn', act_fn)] if act else []\n",
        "        if bn_layer:\n",
        "            bn = nn.BatchNorm2d(nf)\n",
        "            nn.init.constant_(bn.weight, 0. if zero_bn else 1.)\n",
        "            act_bn += [('bn', bn)]\n",
        "        if bn_1st: act_bn.reverse()\n",
        "        layers += act_bn\n",
        "        super().__init__(OrderedDict(layers))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVLFz9nhBz0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NewResBlock(Module):\n",
        "    def __init__(self, expansion, ni, nh, stride=1,\n",
        "                 conv_layer=ConvLayer, act_fn=act_fn, zero_bn=True, bn_1st=True,\n",
        "                 pool=nn.AvgPool2d(2, ceil_mode=True), sa=False, sym=False, groups=1):\n",
        "        nf,ni = nh*expansion,ni*expansion\n",
        "        conv_layer = NewLayer\n",
        "        self.reduce = noop if stride==1 else pool\n",
        "        layers  = [# (f\"conv_0\", conv_layer(ni, nh, 3, act_fn=act_fn, bn_1st=bn_1st)),\n",
        "                   (f\"conv_1\", conv_layer(ni, nf, 3, zero_bn=zero_bn, act=False, bn_layer=True))\n",
        "        ] if expansion == 1 else [\n",
        "                   (f\"conv_0\", conv_layer(ni, nh, 1, act_fn=act_fn, bn_1st=bn_1st)),\n",
        "                   # (f\"conv_1\", conv_layer(nh, nh, 3, act_fn=act_fn, bn_1st=bn_1st)),\n",
        "                   # (f\"conv_1\", conv_layer(nh, nh*dm, 3, groups=nh, act_fn=act_fn, bn_1st=bn_1st)),\n",
        "                   (f\"conv_1\", conv_layer(nh, nh*dm, 3, groups=nh, act=False, bn_layer=False)),\n",
        "                   (f\"conv_2\", conv_layer(nh*dm, nf, 1, zero_bn=zero_bn, act=False, bn_1st=bn_1st))\n",
        "        ]\n",
        "        if sa: layers.append(('sa', SimpleSelfAttention(nf,ks=1,sym=sym)))\n",
        "        self.convs = nn.Sequential(OrderedDict(layers))\n",
        "        self.idconv = noop if ni==nf else conv_layer(ni, nf, 1, act=False, bn_1st=bn_1st)\n",
        "        self.merge = act_fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        o = self.reduce(x)\n",
        "        return self.merge(self.convs(o) + self.idconv(o))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XroIp4GcBz07",
        "colab_type": "text"
      },
      "source": [
        "# Model Constructor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTrZVV81Bz1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Net(c_out=10, layers=[3,6,8,3], expansion=4)\n",
        "model.block = NewResBlock\n",
        "model.conv_layer = NewLayer # for the stem\n",
        "pool = MaxBlurPool2d(3, True)\n",
        "model.pool = pool\n",
        "model.stem_sizes = [3,32,64,64]\n",
        "model.act_fn = Mish()\n",
        "model.sa = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCDxE73yuq_5",
        "colab_type": "text"
      },
      "source": [
        "## Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JfDW5iqPVE4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        },
        "outputId": "d4aac895-03f4-48f0-e9d8-c5072c33eb07"
      },
      "source": [
        "dm = 4\n",
        "res = dict()\n",
        "for e in [5]*5 + [20]*5 + [80]:\n",
        "  mixup=0 if e<=20 else 0.2\n",
        "  learn = get_learn(model=model, size=192, bs=16, mixup=mixup)\n",
        "  learn.fit_fc(e, lr=4e-3, moms=(0.95,0.95), start_pct=0.72)\n",
        "  acc = learn.recorder.metrics[-1][0].item()\n",
        "  if e in res:\n",
        "    res[e] += [acc]\n",
        "  else:\n",
        "    res[e] = [acc]\n",
        "  print('{} epochs: {} ({} runs)'.format(e, sum(res[e])/len(res[e]), len(res[e])))\n",
        "print('depth multiplier={}'.format(dm), {e: sum(res[e])/len(res[e]) for e in res})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data path   /root/.fastai/data/imagewoof2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Learn path /root/.fastai/data/imagewoof2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>top_k_accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.938423</td>\n",
              "      <td>1.738842</td>\n",
              "      <td>0.444388</td>\n",
              "      <td>0.895902</td>\n",
              "      <td>02:58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.614446</td>\n",
              "      <td>1.593913</td>\n",
              "      <td>0.535760</td>\n",
              "      <td>0.921354</td>\n",
              "      <td>02:58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.406542</td>\n",
              "      <td>1.312408</td>\n",
              "      <td>0.671163</td>\n",
              "      <td>0.955205</td>\n",
              "      <td>02:58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.285317</td>\n",
              "      <td>1.198211</td>\n",
              "      <td>0.723848</td>\n",
              "      <td>0.967422</td>\n",
              "      <td>02:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.090849</td>\n",
              "      <td>1.036145</td>\n",
              "      <td>0.798422</td>\n",
              "      <td>0.980657</td>\n",
              "      <td>02:59</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/imagenette_experiments/train_utils.py:150: UserWarning: This overload of addcmul_ is deprecated:\n",
            "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
            "Consider using one of the following signatures instead:\n",
            "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5 epochs: 0.7984219789505005 (1 runs)\n",
            "data path   /root/.fastai/data/imagewoof2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Learn path /root/.fastai/data/imagewoof2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='3' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      60.00% [3/5 08:58<05:59]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>top_k_accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.884023</td>\n",
              "      <td>1.835541</td>\n",
              "      <td>0.425045</td>\n",
              "      <td>0.888521</td>\n",
              "      <td>02:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.651628</td>\n",
              "      <td>1.466147</td>\n",
              "      <td>0.588954</td>\n",
              "      <td>0.939679</td>\n",
              "      <td>02:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.440411</td>\n",
              "      <td>1.368478</td>\n",
              "      <td>0.638330</td>\n",
              "      <td>0.958514</td>\n",
              "      <td>02:59</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='504' class='' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      89.36% [504/564 02:13<00:15 1.2751]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}